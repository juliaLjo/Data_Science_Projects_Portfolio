{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Project-description\" data-toc-modified-id=\"Project-description-1\">Project description</a></span></li><li><span><a href=\"#Getting-to-know-data\" data-toc-modified-id=\"Getting-to-know-data-2\">Getting to know data</a></span></li><li><span><a href=\"#Model-selection\" data-toc-modified-id=\"Model-selection-3\">Model selection</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Decision-Tree-Classifier\" data-toc-modified-id=\"Decision-Tree-Classifier-3.0.1\">Decision Tree Classifier</a></span></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-3.0.2\">Random Forest Classifier</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.0.3\">Logistic Regression</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-3.0.4\">Conclusion</a></span></li></ul></li></ul></li><li><span><a href=\"#Accuracy-test-with-test-dataset\" data-toc-modified-id=\"Accuracy-test-with-test-dataset-4\">Accuracy test with test dataset</a></span></li><li><span><a href=\"#Sanity-check\" data-toc-modified-id=\"Sanity-check-5\">Sanity check</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\">Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobile carrier Megaline has found out that many of their subscribers use legacy plans. They want to develop a model that would analyze subscribers' behavior and recommend one of Megaline's newer plans: Smart or Ultra.<br>\n",
    "The dataset has behavior data about subscribers who have already switched to the new plans. The assumption is that the data doesn't need preparation.<br>\n",
    "<b>The task</b> is to develop a model with the highest possible accuracy. In this project, the\n",
    "threshold for accuracy is 0.75.<br>\n",
    "<b>Data description</b>\n",
    "Every observation in the dataset contains monthly behavior information about one user. The information given is as follows:<br>\n",
    "<i>сalls</i> — number of calls<br>\n",
    "<i>minutes</i> — total call duration in minutes<br>\n",
    "<i>messages</i> — number of text messages<br>\n",
    "<i>mb_used</i> — Internet traffic used in MB<br>\n",
    "<i>is_ultra</i> — plan for the current month (Ultra - 1, Smart - 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to know data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "       warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('users_behavior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>69.0</td>\n",
       "      <td>538.58</td>\n",
       "      <td>64.0</td>\n",
       "      <td>28639.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>98.0</td>\n",
       "      <td>677.18</td>\n",
       "      <td>46.0</td>\n",
       "      <td>16285.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>48.0</td>\n",
       "      <td>369.40</td>\n",
       "      <td>106.0</td>\n",
       "      <td>4520.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>42.0</td>\n",
       "      <td>285.25</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11348.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>17.0</td>\n",
       "      <td>136.21</td>\n",
       "      <td>48.0</td>\n",
       "      <td>24729.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      calls  minutes  messages   mb_used  is_ultra\n",
       "2214   69.0   538.58      64.0  28639.49         0\n",
       "65     98.0   677.18      46.0  16285.67         0\n",
       "822    48.0   369.40     106.0   4520.18         0\n",
       "3015   42.0   285.25      34.0  11348.42         0\n",
       "1789   17.0   136.21      48.0  24729.33         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change value of the columns \"calls\" and \"messages\" to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['calls'] = df['calls'].astype('int')\n",
    "df['messages'] = df['messages'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for duplicates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: the dataset has 3214 observations and 5 features. There is no missing or duplicated values. Our target is \"is_ultra\" column. We converted values in some columns to integer and left others as is, e.g. float type, in order to preserve this information for better model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into training, validation and test sets in the following ratio: 3:1:1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1928, 5) (643, 5) (643, 5)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into 80% and 20%\n",
    "remainder, test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "# splitting 80% into 75% and 25%, thus getting 60% and 20% of total respectively\n",
    "train, valid = train_test_split(remainder, test_size=0.25, random_state=1)\n",
    "print(train.shape, test.shape, valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = train.drop(['is_ultra'], axis=1)\n",
    "features_test = test.drop(['is_ultra'], axis=1)\n",
    "features_valid = valid.drop(['is_ultra'], axis=1)\n",
    "target_train = train['is_ultra']\n",
    "target_test = test['is_ultra']\n",
    "target_valid = valid['is_ultra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider Decision Tree Classifier model. We will compare the quality of the model for different values of the 'max_depth' hyper-parameter, which is the maximum depth of the tree.<br>\n",
    "Note: Quality of the model is the number of correct answers to the total number of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree depth 1\n",
      "Model score on the training set   0.768\n",
      "Model score on the validation set 0.742\n",
      "Tree depth 2\n",
      "Model score on the training set   0.794\n",
      "Model score on the validation set 0.762\n",
      "Tree depth 3\n",
      "Model score on the training set   0.805\n",
      "Model score on the validation set 0.785\n",
      "Tree depth 4\n",
      "Model score on the training set   0.821\n",
      "Model score on the validation set 0.795\n",
      "Tree depth 5\n",
      "Model score on the training set   0.828\n",
      "Model score on the validation set 0.792\n"
     ]
    }
   ],
   "source": [
    "for depth in range(1, 6):\n",
    "    model = DecisionTreeClassifier(random_state=123, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    print('Tree depth', depth)\n",
    "    print('Model score on the training set  ', '{:.3}'.format(model.score(features_train, target_train)))\n",
    "    print('Model score on the validation set', '{:.3}'.format(model.score(features_valid, target_valid)))\n",
    "    #print('Precision / recall pair:', precision_recall_curve(target_test, model.predict(features_test)), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model quality was achieved with the 'max_depth' parameter set to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model that we will try out is the Random Forest Classifier. For this model we will examine the model quality vs. different values of the 'n-estimators' hyper-parameter, which is the number of trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected \"n-estimators\" hyper-parameter: 1\n",
      "Model score on the training set   0.912\n",
      "Model score on the validation set 0.72\n",
      "Selected \"n-estimators\" hyper-parameter: 5\n",
      "Model score on the training set   0.97\n",
      "Model score on the validation set 0.77\n",
      "Selected \"n-estimators\" hyper-parameter: 9\n",
      "Model score on the training set   0.988\n",
      "Model score on the validation set 0.792\n",
      "Selected \"n-estimators\" hyper-parameter: 13\n",
      "Model score on the training set   0.993\n",
      "Model score on the validation set 0.796\n",
      "Selected \"n-estimators\" hyper-parameter: 17\n",
      "Model score on the training set   0.994\n",
      "Model score on the validation set 0.804\n",
      "Selected \"n-estimators\" hyper-parameter: 21\n",
      "Model score on the training set   0.997\n",
      "Model score on the validation set 0.793\n",
      "Selected \"n-estimators\" hyper-parameter: 25\n",
      "Model score on the training set   0.997\n",
      "Model score on the validation set 0.804\n",
      "Selected \"n-estimators\" hyper-parameter: 29\n",
      "Model score on the training set   0.998\n",
      "Model score on the validation set 0.795\n"
     ]
    }
   ],
   "source": [
    "for estim in range(1, 30, 4):\n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators=estim)\n",
    "    model.fit(features_train, target_train)\n",
    "    print('Selected \"n-estimators\" hyper-parameter:', estim)\n",
    "    print('Model score on the training set  ', '{:.3}'.format(model.score(features_train, target_train)))\n",
    "    print('Model score on the validation set', '{:.3}'.format(model.score(features_valid, target_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model quality was achieved with the 'n_estimators' parameter set to 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another model that we will try is Logistic Regression. We will iterate through possible values for the 'solver' hyper-parameter, which is an algorithm that the model uses in the optimization problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected \"solver\" hyper-parameter: newton-cg\n",
      "Model score on the training set   0.751\n",
      "Model score on the validation set 0.756\n",
      "Selected \"solver\" hyper-parameter: saga\n",
      "Model score on the training set   0.707\n",
      "Model score on the validation set 0.68\n",
      "Selected \"solver\" hyper-parameter: sag\n",
      "Model score on the training set   0.707\n",
      "Model score on the validation set 0.68\n",
      "Selected \"solver\" hyper-parameter: lbfgs\n",
      "Model score on the training set   0.751\n",
      "Model score on the validation set 0.756\n",
      "Selected \"solver\" hyper-parameter: liblinear\n",
      "Model score on the training set   0.751\n",
      "Model score on the validation set 0.75\n"
     ]
    }
   ],
   "source": [
    "for solver in {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}:\n",
    "    model = LogisticRegression(random_state=123, solver=solver)\n",
    "    model.fit(features_train, target_train)\n",
    "    print('Selected \"solver\" hyper-parameter:', solver)\n",
    "    print('Model score on the training set  ', '{:.3}'.format(model.score(features_train, target_train)))\n",
    "    print('Model score on the validation set', '{:.3}'.format(model.score(features_valid, target_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was achieved for both 'newton-cg' and lbfgs' algorithms for the 'solver' hyper-parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the three models that we have tried the best accuracy was achieved with the Random Forest Classifier model with the number of trees in the forest set to 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy test with test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the selected model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on the test set   0.801\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345, n_estimators=25)\n",
    "model.fit(features_train, target_train)\n",
    "print('Model score on the test set  ', '{:.3}'.format(model.score(features_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model got 0.80 score, which satisfy our threshold of 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform sanity check on a randomly selected model to assess whether the Random Forest Classifier model makes sense. We will choose the Naive Bayes Classification model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on the training set   0.789\n",
      "Model score on the validation set 0.785\n",
      "Model score on the test set       0.778\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(features_train, target_train)\n",
    "print('Model score on the training set  ', '{:.3}'.format(model.score(features_train, target_train)))\n",
    "print('Model score on the validation set', '{:.3}'.format(model.score(features_valid, target_valid)))\n",
    "print('Model score on the test set      ', '{:.3}'.format(model.score(features_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores we got are pretty decent and are in line with the scores we were getting before with other models, and specifically with the Random Forest Classifier that we chose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the three chosen models the <b>Logistic Regression</b> is the least accurate and barely pass our 75% accuracy threshold. The <b>Random Forest Classifier</b> shows the most accuracy at the \"n_estimators\" parameter set to 25. The <b>Decision Tree Classifier</b> model has tolerable accuracy level above the 0.75 threshold at the \"max_depth\" hyper-parameter set at 2 and above. The highest accuracy was observed at with the tree depth set to 5.<br>\n",
    "We will select the Random Forest Classifier model, even thought it's a slower model than the Decision Tree Classifier, but in this exercise the speed was not set as a limiting factor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
